# ğŸ’¬ Real-time LLM Chat App

A real-time chat interface powered by **Streamlit**, **FastAPI**, **Socket.IO**, and **Ollama (or OpenAI-compatible LLMs)**. It allows multiple users to interact with an LLM backend seamlessly through a browser-based UI.

---

## âš™ï¸ Tech Stack

- **Frontend:** Streamlit + Python Socket.IO Client  
- **Backend:** FastAPI + Python Socket.IO Server  
- **LLM Engine:** Local LLM via [Ollama](https://ollama.com/) or OpenAI-compatible APIs  
- **Containerization:** Docker + Docker Compose

---

## ğŸ“ Project Structure

```
realtime-llm-chat-app/
â”œâ”€â”€ backend/                 # FastAPI backend with LLM and Socket.IO
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ frontend/                # Streamlit frontend UI with real-time chat
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml       # Compose file to run frontend & backend
â”œâ”€â”€ Dockerfile.backend       # Backend Dockerfile
â”œâ”€â”€ Dockerfile.frontend      # Frontend Dockerfile
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### ğŸ› ï¸ Prerequisites

- Docker and Docker Compose installed  
- [Ollama](https://ollama.com/download) installed (for local LLMs) â€” optional if using OpenAI

---

### ğŸ³ Run with Docker

```bash
docker compose build
docker compose up
```

- Access frontend: [http://localhost:8501](http://localhost:8501)  
- Backend API: [http://localhost:8000](http://localhost:8000)

---

## ğŸ”Œ How It Works

- Frontend connects to backend via **Socket.IO**
- Messages are sent in real-time to the FastAPI backend
- Backend queries an LLM (Ollama or OpenAI) and returns responses
- All services are containerized and run together via Docker Compose

---

## âœ… Features

- Real-time WebSocket chat interface  
- Supports multiple clients simultaneously  
- Pluggable LLM backend (Ollama/OpenAI/etc.)  
- Clean UI built with Streamlit  
- Cross-platform, Dockerized setup

---

## ğŸ§  Future Improvements

- Add authentication / user sessions  
- Store chat history using a database  
- Improve error handling and reconnects  
- Support advanced LLM parameters (temperature, max tokens, etc.)

---

## ğŸ™‹â€â™‚ï¸ Author

**Akshay Mehta**  
ğŸ”— [GitHub Profile](https://github.com/Mrakshaymehta)

---

## ğŸ“ License

MIT License â€“ free to use and modify
